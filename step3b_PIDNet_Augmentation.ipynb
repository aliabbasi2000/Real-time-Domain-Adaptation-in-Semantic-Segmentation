{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeAUadT0X_nw",
        "outputId": "a2666280-74de-4a1e-c4b1-b07af3392826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of available CPU cores: 1\n",
            "Number of available GPUs: 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "num_cpus = torch.get_num_threads()\n",
        "\n",
        "print(f\"Number of available CPU cores: {num_cpus}\")\n",
        "print(f\"Number of available GPUs: {num_gpus}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aliabbasi2000/PIDNet.git\n",
        "%cd /content/PIDNet/"
      ],
      "metadata": {
        "id": "3WiSblceYU0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/5706578/files/Train.zip\n",
        "!wget https://zenodo.org/record/5706578/files/Val.zip\n",
        "!unzip Train.zip -d ./LoveDA\n",
        "!unzip Val.zip -d ./LoveDA"
      ],
      "metadata": {
        "id": "PNoumo0wYbWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --content-disposition \"https://drive.usercontent.google.com/u/0/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-&export=download\"\n",
        "!mv PIDNet_S_ImageNet.pth.tar /content/PIDNet/pretrained_models/imagenet"
      ],
      "metadata": {
        "id": "IexvI0draozB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from albumentations import Compose, Normalize, Resize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "class LoveDADataset(Dataset):\n",
        "    def __init__(self, root, split='train', region='urban', transform=None):\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.region = region\n",
        "        self.image_dir = os.path.join(root, split, region, 'images_png')\n",
        "        self.mask_dir = os.path.join(root, split, region, 'masks_png')\n",
        "        self.images = sorted(os.listdir(self.image_dir))\n",
        "        self.masks = sorted(os.listdir(self.mask_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      image_path = os.path.join(self.image_dir, self.images[idx])\n",
        "      mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
        "\n",
        "      # Load the image and mask\n",
        "      image = cv2.imread(image_path)\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "      mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "      # Convert mask to tensor and shift labels\n",
        "      mask = np.array(mask, dtype=np.int64) - 1  # Shift labels to range [0, 6]\n",
        "      mask = np.clip(mask, 0, num_classes - 1)  # Ensure no negative values\n",
        "\n",
        "      # Apply transformations if specified\n",
        "      if self.transform:\n",
        "          augmented = self.transform(image=image, mask=mask)\n",
        "          image = augmented[\"image\"]\n",
        "          mask = augmented[\"mask\"]\n",
        "\n",
        "      # Convert to PyTorch tensors\n",
        "      image = image.clone().detach().float()  # HWC to CHW\n",
        "      mask = mask.clone().detach().long()\n",
        "\n",
        "      return image, mask\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InTjjTXcarR6",
        "outputId": "5b957f24-900f-4d80-cc14-60db3dec9579"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from albumentations import Compose, HorizontalFlip, RandomRotate90, RandomCrop, VerticalFlip, Normalize, OneOf, NoOp, GaussianBlur, RandomBrightnessContrast, MultiplicativeNoise\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define transforms for training phase\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "train_transform = Compose([\n",
        "    Resize(512, 512),  # Resize both image and mask\n",
        "    # Augmentation\n",
        "    OneOf([\n",
        "      HorizontalFlip(p=0.5),\n",
        "      VerticalFlip(p=0.5),\n",
        "      RandomRotate90(p=0.5),\n",
        "      GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
        "      RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),  # Adjust brightness and contrast\n",
        "      MultiplicativeNoise(multiplier=(0.9, 1.1), p=0.5),\n",
        "\n",
        "      NoOp()  # do no augmentation some times\n",
        "    ], p=0.75),  # The entire block is applied with 75% probability\n",
        "\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], always_apply=True),  # Normalize\n",
        "    ToTensorV2()  # Convert to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Define transforms for the evaluation phase\n",
        "\n",
        "eval_transform = Compose([\n",
        "    Resize(512, 512),  # Resize to match the input size of the model\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], always_apply=True),  # Normalize with ImageNet stats\n",
        "    ToTensorV2()  # Convert to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Define dataset paths\n",
        "data_root = '/content/PIDNet/LoveDA'\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = LoveDADataset(root=data_root, split='Train', region='Urban', transform=train_transform)\n",
        "val_dataset = LoveDADataset(root=data_root, split='Val', region='Rural', transform=eval_transform)\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "M-AbNLVSavQs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.pidnet import get_seg_model\n",
        "import yaml\n",
        "\n",
        "## get_pred ------------------------------------\n",
        "#model = get_pred_model('pidnet_s', num_classes=7)\n",
        "\n",
        "## get_seg ------------------------------------\n",
        "# Convert the dictionary to a DotDict\n",
        "class DotDict(dict):\n",
        "    \"\"\"A dictionary that supports attribute-style access.\"\"\"\n",
        "    def __getattr__(self, name):\n",
        "        value = self.get(name)\n",
        "        if isinstance(value, dict):\n",
        "            return DotDict(value)\n",
        "        return value\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        self[name] = value\n",
        "\n",
        "# Load the YAML configuration\n",
        "with open('/content/PIDNet/configs/loveda/pidnet_small_loveda.yaml', 'r') as f:\n",
        "    cfg_dict = yaml.safe_load(f)\n",
        "\n",
        "# Convert to DotDict for attribute-style access\n",
        "cfg = DotDict(cfg_dict)\n",
        "\n",
        "imgnet = 'imagenet' in cfg.MODEL.PRETRAINED\n",
        "\n",
        "model = get_seg_model(cfg, imgnet_pretrained=imgnet)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WE8tNcbth2pc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "j_GNFvvfh9Gl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def calculate_iou(predictions, ground_truth, num_classes):\n",
        "    classwise_iou = []\n",
        "    for cls in range(num_classes):\n",
        "        tp = ((predictions == cls) & (ground_truth == cls)).sum().item()\n",
        "        fp = ((predictions == cls) & (ground_truth != cls)).sum().item()\n",
        "        fn = ((predictions != cls) & (ground_truth == cls)).sum().item()\n",
        "\n",
        "        if tp + fp + fn == 0:\n",
        "            iou = float('nan')\n",
        "        else:\n",
        "            iou = tp / (tp + fp + fn)\n",
        "        classwise_iou.append(iou)\n",
        "\n",
        "    return classwise_iou\n",
        "\n",
        "# Training\n",
        "model.train()\n",
        "num_epochs = 20\n",
        "num_classes = 7\n",
        "class_labels = [\"background\", \"building\", \"road\", \"water\", \"barren\", \"forest\", \"agriculture\"]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Assuming 'outputs' is a list and the desired output is the first element:\n",
        "        outputs = outputs[0]  # Select the first element of the list\n",
        "\n",
        "        # Resize masks to match outputs in size= ...\n",
        "        masks = F.interpolate(masks.unsqueeze(1).float(), size=(outputs.shape[2], outputs.shape[3]), mode='nearest').squeeze(1).long()\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
        "    torch.save(model.state_dict(), 'step3a_trained_on_urban_eval_on_rural.pth')\n",
        "\n",
        "\n",
        "# Validation phase\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "val_classwise_iou = np.zeros(num_classes)\n",
        "num_batches = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for images, masks in val_loader:\n",
        "    images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "\n",
        "    outputs = outputs[0]\n",
        "    # Resize masks to match outputs size=?????\n",
        "    masks = F.interpolate(masks.unsqueeze(1).float(), size=(outputs.shape[2], outputs.shape[3]), mode='nearest').squeeze(1).long() # Resize masks to match outputs\n",
        "    loss = criterion(outputs, masks)\n",
        "    val_loss += loss.item()\n",
        "\n",
        "    # Compute class-wise IoU\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "    batch_iou = calculate_iou(preds, masks, num_classes)\n",
        "    val_classwise_iou += np.nan_to_num(batch_iou)  # Accumulate IoU for each class\n",
        "    num_batches += 1\n",
        "\n",
        "val_loss /= len(val_loader)\n",
        "mean_classwise_iou = val_classwise_iou / num_batches\n",
        "mean_iou = np.nanmean(mean_classwise_iou)  # Overall mIoU\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation mIoU: {mean_iou:.4f}\")\n",
        "print(\"Class-wise IoU:\")\n",
        "for cls, label in enumerate(class_labels):\n",
        "    print(f\"  {label}: {mean_classwise_iou[cls]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5xDoLkCiAQ5",
        "outputId": "3493c51f-0393-464b-adbb-c9199445cea3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Training Loss: 2.9416\n",
            "Epoch 2/20, Training Loss: 1.1593\n",
            "Epoch 3/20, Training Loss: 0.9933\n",
            "Epoch 4/20, Training Loss: 0.9188\n",
            "Epoch 5/20, Training Loss: 0.8741\n",
            "Epoch 6/20, Training Loss: 0.8405\n",
            "Epoch 7/20, Training Loss: 0.8119\n",
            "Epoch 8/20, Training Loss: 0.7856\n",
            "Epoch 9/20, Training Loss: 0.7711\n",
            "Epoch 10/20, Training Loss: 0.7543\n",
            "Epoch 11/20, Training Loss: 0.7349\n",
            "Epoch 12/20, Training Loss: 0.7246\n",
            "Epoch 13/20, Training Loss: 0.7174\n",
            "Epoch 14/20, Training Loss: 0.7083\n",
            "Epoch 15/20, Training Loss: 0.6895\n",
            "Epoch 16/20, Training Loss: 0.6813\n",
            "Epoch 17/20, Training Loss: 0.6680\n",
            "Epoch 18/20, Training Loss: 0.6676\n",
            "Epoch 19/20, Training Loss: 0.6601\n",
            "Epoch 20/20, Training Loss: 0.6448\n",
            "Validation Loss: 1.9452, Validation mIoU: 0.2675\n",
            "Class-wise IoU:\n",
            "  background: 0.4959\n",
            "  building: 0.3543\n",
            "  road: 0.2131\n",
            "  water: 0.2966\n",
            "  barren: 0.0914\n",
            "  forest: 0.1627\n",
            "  agriculture: 0.2583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Augmantation7: all Augs except GaussianBlur\n",
        "\n",
        "Validation Loss: 1.9452, Validation mIoU: 0.2675\n",
        "\n",
        "Class-wise IoU:\n",
        "  background: 0.4959\n",
        "  building: 0.3543\n",
        "  road: 0.2131\n",
        "  water: 0.2966\n",
        "  barren: 0.0914\n",
        "  forest: 0.1627\n",
        "  agriculture: 0.2583"
      ],
      "metadata": {
        "id": "LmIb8xPRPZZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Augmentation6: Only RandomRotate90\n",
        "\n",
        "Validation Loss: 1.8232, Validation mIoU: 0.2637\n",
        "\n",
        "Class-wise IoU:\n",
        "  background: 0.4943\n",
        "  building: 0.3063\n",
        "  road: 0.2164\n",
        "  water: 0.3013\n",
        "  barren: 0.1157\n",
        "  forest: 0.1566\n",
        "  agriculture: 0.2550"
      ],
      "metadata": {
        "id": "sLeN7Y4_KN_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Augmentation5: Only GaussianBlur\n",
        "\n",
        "Validation Loss: 1.9924, Validation mIoU: 0.2472\n",
        "\n",
        "Class-wise IoU:\n",
        "  background: 0.5000\n",
        "  building: 0.3200\n",
        "  road: 0.2273\n",
        "  water: 0.3239\n",
        "  barren: 0.0900\n",
        "  forest: 0.0731\n",
        "  agriculture: 0.1965"
      ],
      "metadata": {
        "id": "gjfXizJJvkCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Augmentation4: OneOf([HorizontalFlip(p=0.5), VerticalFlip(p=0.5), RandomRotate90(p=0.5), GaussianBlur(blur_limit=(3, 7), p=0.5),RandomBrightnessContras(brightness_limit=0.2, contrast_limit=0.2, p=0.5),MultiplicativeNoise(multiplier=(0.9, 1.1), p=0.5),NoOp()], p=0.75),\n",
        "\n",
        "Epoch 1/20, Training Loss: 2.7239\n",
        "\n",
        "Epoch 2/20, Training Loss: 1.1570\n",
        "\n",
        "Epoch 3/20, Training Loss: 0.9961\n",
        "\n",
        "Epoch 4/20, Training Loss: 0.9187\n",
        "\n",
        "Epoch 5/20, Training Loss: 0.8634\n",
        "\n",
        "Epoch 6/20, Training Loss: 0.8490\n",
        "\n",
        "Epoch 7/20, Training Loss: 0.8249\n",
        "\n",
        "Epoch 8/20, Training Loss: 0.7929\n",
        "\n",
        "Epoch 9/20, Training Loss: 0.7793\n",
        "\n",
        "Epoch 10/20, Training Loss: 0.7572\n",
        "\n",
        "Epoch 11/20, Training Loss: 0.7410\n",
        "\n",
        "Epoch 12/20, Training Loss: 0.7412\n",
        "\n",
        "Epoch 13/20, Training Loss: 0.7206\n",
        "\n",
        "Epoch 14/20, Training Loss: 0.7119\n",
        "\n",
        "Epoch 15/20, Training Loss: 0.7008\n",
        "\n",
        "Epoch 16/20, Training Loss: 0.6842\n",
        "\n",
        "Epoch 17/20, Training Loss: 0.6835\n",
        "\n",
        "Epoch 18/20, Training Loss: 0.6785\n",
        "\n",
        "Epoch 19/20, Training Loss: 0.6655\n",
        "\n",
        "Epoch 20/20, Training Loss: 0.6561\n",
        "\n",
        "Validation Loss: 1.9382, Validation mIoU: 0.2736\n",
        "\n",
        "Class-wise IoU:\n",
        "  background: 0.5085\n",
        "  building: 0.3271\n",
        "  road: 0.2415\n",
        "  water: 0.2559\n",
        "  barren: 0.0853\n",
        "  forest: 0.1348\n",
        "  agriculture: 0.3621"
      ],
      "metadata": {
        "id": "gbxJpSn3onRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentation3: RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),MultiplicativeNoise(multiplier=(0.9, 1.1), p=0.5),\n",
        "\n",
        "Validation Loss: 1.9632, Validation mIoU: 0.2383\n",
        "\n",
        "Class-wise IoU:\n",
        "  background: 0.4722\n",
        "  building: 0.3635\n",
        "  road: 0.1996\n",
        "  water: 0.2604\n",
        "  barren: 0.0809\n",
        "  forest: 0.0923\n",
        "  agriculture: 0.1994"
      ],
      "metadata": {
        "id": "CfuV2XAgcU-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentation2: OneOf([HorizontalFlip(p=0.5), VerticalFlip(p=0.5), RandomRotate90(p=0.5),GaussianBlur(blur_limit=(3, 7), p=0.5), NoOp()], p=0.75),\n",
        "\n",
        "\n",
        "Epoch 1/20, Training Loss: 2.2140\n",
        "\n",
        "Epoch 2/20, Training Loss: 1.0422\n",
        "\n",
        "Epoch 3/20, Training Loss: 0.9286\n",
        "\n",
        "Epoch 4/20, Training Loss: 0.8790\n",
        "\n",
        "Epoch 5/20, Training Loss: 0.8348\n",
        "\n",
        "Epoch 6/20, Training Loss: 0.8201\n",
        "\n",
        "Epoch 7/20, Training Loss: 0.7918\n",
        "\n",
        "Epoch 8/20, Training Loss: 0.7691\n",
        "\n",
        "Epoch 9/20, Training Loss: 0.7515\n",
        "\n",
        "Epoch 10/20, Training Loss: 0.7390\n",
        "\n",
        "Epoch 11/20, Training Loss: 0.7260\n",
        "\n",
        "Epoch 12/20, Training Loss: 0.7160\n",
        "\n",
        "Epoch 13/20, Training Loss: 0.6964\n",
        "\n",
        "Epoch 14/20, Training Loss: 0.7039\n",
        "\n",
        "Epoch 15/20, Training Loss: 0.6896\n",
        "\n",
        "Epoch 16/20, Training Loss: 0.6878\n",
        "\n",
        "Epoch 17/20, Training Loss: 0.6713\n",
        "\n",
        "Epoch 18/20, Training Loss: 0.6693\n",
        "\n",
        "Epoch 19/20, Training Loss: 0.6601\n",
        "\n",
        "Epoch 20/20, Training Loss: 0.6513\n",
        "\n",
        "Validation Loss: 1.7113, Validation mIoU: 0.2669\n",
        "\n",
        "Class-wise IoU:\n",
        "  background: 0.4945\n",
        "  building: 0.3915\n",
        "  road: 0.2166\n",
        "  water: 0.3269\n",
        "  barren: 0.0921\n",
        "  forest: 0.0876\n",
        "  agriculture: 0.2590"
      ],
      "metadata": {
        "id": "YaCztT38x40w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentation1: Only RandomRotate90(p=0.5), GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
        "\n",
        "Validation Loss: 1.8494, Validation mIoU: 0.2513\n",
        "Class-wise IoU:\n",
        "  background: 0.4961\n",
        "  building: 0.3070\n",
        "  road: 0.2063\n",
        "  water: 0.3012\n",
        "  barren: 0.0636\n",
        "  forest: 0.1090\n",
        "  agriculture: 0.2762"
      ],
      "metadata": {
        "id": "FINLQx1To1LC"
      }
    }
  ]
}