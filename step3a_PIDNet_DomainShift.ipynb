{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y_Ff1DC8MRLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb55563a-1aaa-45db-94ff-e19165ad5e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of available CPU cores: 1\n",
            "Number of available GPUs: 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "num_cpus = torch.get_num_threads()\n",
        "\n",
        "print(f\"Number of available CPU cores: {num_cpus}\")\n",
        "print(f\"Number of available GPUs: {num_gpus}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision albumentations"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tRydukZvNPA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aliabbasi2000/PIDNet.git\n",
        "%cd /content/PIDNet/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFGr_XA4NZ2g",
        "outputId": "b09ba5b0-a8c0-499b-9882-1c370df8528e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PIDNet'...\n",
            "remote: Enumerating objects: 585, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 585 (delta 11), reused 21 (delta 10), pack-reused 558 (from 1)\u001b[K\n",
            "Receiving objects: 100% (585/585), 246.53 MiB | 22.94 MiB/s, done.\n",
            "Resolving deltas: 100% (239/239), done.\n",
            "/content/PIDNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/5706578/files/Train.zip\n",
        "!wget https://zenodo.org/record/5706578/files/Val.zip\n",
        "!unzip Train.zip -d ./LoveDA\n",
        "!unzip Val.zip -d ./LoveDA"
      ],
      "metadata": {
        "id": "XAt7NwwrNeao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --content-disposition \"https://drive.usercontent.google.com/u/0/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-&export=download\"\n",
        "!mv PIDNet_S_ImageNet.pth.tar /content/PIDNet/pretrained_models/imagenet"
      ],
      "metadata": {
        "id": "JJHSJc5PPgyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from albumentations import Compose, Normalize, Resize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "class LoveDADataset(Dataset):\n",
        "    def __init__(self, root, split='train', region='urban', transform=None):\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.region = region\n",
        "        self.image_dir = os.path.join(root, split, region, 'images_png')\n",
        "        self.mask_dir = os.path.join(root, split, region, 'masks_png')\n",
        "        self.images = sorted(os.listdir(self.image_dir))\n",
        "        self.masks = sorted(os.listdir(self.mask_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      image_path = os.path.join(self.image_dir, self.images[idx])\n",
        "      mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
        "\n",
        "      # Load the image and mask\n",
        "      image = cv2.imread(image_path)\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "      mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "      # Convert mask to tensor and shift labels\n",
        "      mask = np.array(mask, dtype=np.int64) - 1  # Shift labels to range [0, 6]\n",
        "      mask = np.clip(mask, 0, num_classes - 1)  # Ensure no negative values\n",
        "\n",
        "      # Apply transformations if specified\n",
        "      if self.transform:\n",
        "          augmented = self.transform(image=image, mask=mask)\n",
        "          image = augmented[\"image\"]\n",
        "          mask = augmented[\"mask\"]\n",
        "\n",
        "      # Convert to PyTorch tensors\n",
        "      image = image.clone().detach().float()  # HWC to CHW\n",
        "      mask = mask.clone().detach().long()\n",
        "\n",
        "      return image, mask\n"
      ],
      "metadata": {
        "id": "5y0yYBrbPkWP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from albumentations import Compose, HorizontalFlip, RandomRotate90\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define transforms for training phase\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(512, 512),  # Resize both image and mask\n",
        "    #####    AUGMENTATION HERE      #######\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "    ToTensorV2()  # Convert to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Define transforms for the evaluation phase\n",
        "\n",
        "eval_transform = A.Compose([\n",
        "    A.Resize(512, 512),  # Resize to match the input size of the model\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats\n",
        "    ToTensorV2()  # Convert to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Define dataset paths\n",
        "data_root = '/content/PIDNet/LoveDA'\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = LoveDADataset(root=data_root, split='Train', region='Urban', transform=train_transform)\n",
        "val_dataset = LoveDADataset(root=data_root, split='Val', region='Rural', transform=eval_transform)\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "FWsFVCgNPpTk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.pidnet import get_seg_model\n",
        "import yaml\n",
        "\n",
        "## get_pred ------------------------------------\n",
        "#model = get_pred_model('pidnet_s', num_classes=7)\n",
        "\n",
        "## get_seg ------------------------------------\n",
        "# Convert the dictionary to a DotDict\n",
        "class DotDict(dict):\n",
        "    \"\"\"A dictionary that supports attribute-style access.\"\"\"\n",
        "    def __getattr__(self, name):\n",
        "        value = self.get(name)\n",
        "        if isinstance(value, dict):\n",
        "            return DotDict(value)\n",
        "        return value\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        self[name] = value\n",
        "\n",
        "# Load the YAML configuration\n",
        "with open('/content/PIDNet/configs/loveda/pidnet_small_loveda.yaml', 'r') as f:\n",
        "    cfg_dict = yaml.safe_load(f)\n",
        "\n",
        "# Convert to DotDict for attribute-style access\n",
        "cfg = DotDict(cfg_dict)\n",
        "\n",
        "imgnet = 'imagenet' in cfg.MODEL.PRETRAINED\n",
        "\n",
        "model = get_seg_model(cfg, imgnet_pretrained=imgnet)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H8gtDLiQPtlV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "_h1x5X_XPxKS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def calculate_iou(predictions, ground_truth, num_classes):\n",
        "    classwise_iou = []\n",
        "    for cls in range(num_classes):\n",
        "        tp = ((predictions == cls) & (ground_truth == cls)).sum().item()\n",
        "        fp = ((predictions == cls) & (ground_truth != cls)).sum().item()\n",
        "        fn = ((predictions != cls) & (ground_truth == cls)).sum().item()\n",
        "\n",
        "        if tp + fp + fn == 0:\n",
        "            iou = float('nan')\n",
        "        else:\n",
        "            iou = tp / (tp + fp + fn)\n",
        "        classwise_iou.append(iou)\n",
        "\n",
        "    return classwise_iou\n",
        "\n",
        "# Training\n",
        "model.train()\n",
        "num_epochs = 20\n",
        "num_classes = 7\n",
        "class_labels = [\"background\", \"building\", \"road\", \"water\", \"barren\", \"forest\", \"agriculture\"]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Assuming 'outputs' is a list and the desired output is the first element:\n",
        "        outputs = outputs[0]  # Select the first element of the list\n",
        "\n",
        "        # Resize masks to match outputs in size= ...\n",
        "        masks = F.interpolate(masks.unsqueeze(1).float(), size=(outputs.shape[2], outputs.shape[3]), mode='nearest').squeeze(1).long()\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
        "    torch.save(model.state_dict(), 'step3a_trained_on_urban_eval_on_rural.pth')\n",
        "\n",
        "\n",
        "# Validation phase\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "val_classwise_iou = np.zeros(num_classes)\n",
        "num_batches = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for images, masks in val_loader:\n",
        "    images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "\n",
        "    outputs = outputs[0]\n",
        "    # Resize masks to match outputs size=?????\n",
        "    masks = F.interpolate(masks.unsqueeze(1).float(), size=(outputs.shape[2], outputs.shape[3]), mode='nearest').squeeze(1).long() # Resize masks to match outputs\n",
        "    loss = criterion(outputs, masks)\n",
        "    val_loss += loss.item()\n",
        "\n",
        "    # Compute class-wise IoU\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "    batch_iou = calculate_iou(preds, masks, num_classes)\n",
        "    val_classwise_iou += np.nan_to_num(batch_iou)  # Accumulate IoU for each class\n",
        "    num_batches += 1\n",
        "\n",
        "val_loss /= len(val_loader)\n",
        "mean_classwise_iou = val_classwise_iou / num_batches\n",
        "mean_iou = np.nanmean(mean_classwise_iou)  # Overall mIoU\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation mIoU: {mean_iou:.4f}\")\n",
        "print(\"Class-wise IoU:\")\n",
        "for cls, label in enumerate(class_labels):\n",
        "    print(f\"  {label}: {mean_classwise_iou[cls]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQgvGK5-QQGX",
        "outputId": "7dec99b4-1049-49c7-99ec-ce8e04ea1d24"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Training Loss: 2.1118\n",
            "Epoch 2/20, Training Loss: 1.0942\n",
            "Epoch 3/20, Training Loss: 0.9358\n",
            "Epoch 4/20, Training Loss: 0.8998\n",
            "Epoch 5/20, Training Loss: 0.8287\n",
            "Epoch 6/20, Training Loss: 0.7924\n",
            "Epoch 7/20, Training Loss: 0.7597\n",
            "Epoch 8/20, Training Loss: 0.7371\n",
            "Epoch 9/20, Training Loss: 0.7148\n",
            "Epoch 10/20, Training Loss: 0.6898\n",
            "Epoch 11/20, Training Loss: 0.6714\n",
            "Epoch 12/20, Training Loss: 0.6540\n",
            "Epoch 13/20, Training Loss: 0.6419\n",
            "Epoch 14/20, Training Loss: 0.6283\n",
            "Epoch 15/20, Training Loss: 0.6084\n",
            "Epoch 16/20, Training Loss: 0.5979\n",
            "Epoch 17/20, Training Loss: 0.5935\n",
            "Epoch 18/20, Training Loss: 0.5713\n",
            "Epoch 19/20, Training Loss: 0.5564\n",
            "Epoch 20/20, Training Loss: 0.5491\n",
            "Validation Loss: 2.1376, Validation mIoU: 0.2480\n",
            "Class-wise IoU:\n",
            "  background: 0.5108\n",
            "  building: 0.2952\n",
            "  road: 0.2502\n",
            "  water: 0.2469\n",
            "  barren: 0.0663\n",
            "  forest: 0.1004\n",
            "  agriculture: 0.2661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr=0.0001 with get_seg_model and no Augmentaton & Domain Shift(Trained of Urban, Val on Rural)\n",
        "\n",
        "Epoch 1/20, Training Loss: 2.0786\n",
        "\n",
        "Epoch 2/20, Training Loss: 1.0463\n",
        "\n",
        "Epoch 3/20, Training Loss: 0.9087\n",
        "\n",
        "Epoch 4/20, Training Loss: 0.8506\n",
        "\n",
        "Epoch 5/20, Training Loss: 0.7955\n",
        "\n",
        "Epoch 6/20, Training Loss: 0.7593\n",
        "\n",
        "Epoch 7/20, Training Loss: 0.7413\n",
        "\n",
        "Epoch 8/20, Training Loss: 0.7044\n",
        "\n",
        "Epoch 9/20, Training Loss: 0.6886\n",
        "\n",
        "Epoch 10/20, Training Loss: 0.6677\n",
        "\n",
        "Epoch 11/20, Training Loss: 0.6507\n",
        "\n",
        "Epoch 12/20, Training Loss: 0.6294\n",
        "\n",
        "Epoch 13/20, Training Loss: 0.6180\n",
        "\n",
        "Epoch 14/20, Training Loss: 0.6018\n",
        "\n",
        "Epoch 15/20, Training Loss: 0.5814\n",
        "\n",
        "Epoch 16/20, Training Loss: 0.5706\n",
        "\n",
        "Epoch 17/20, Training Loss: 0.5618\n",
        "\n",
        "Epoch 18/20, Training Loss: 0.5462\n",
        "\n",
        "Epoch 19/20, Training Loss: 0.5390\n",
        "\n",
        "Epoch 20/20, Training Loss: 0.5177\n",
        "\n",
        "Validation Loss: 1.9366, Validation mIoU: 0.\n",
        "2467\n",
        "Class-wise IoU:\n",
        "  background: 0.5047\n",
        "  building: 0.2810\n",
        "  road: 0.2278\n",
        "  water: 0.2999\n",
        "  barren: 0.0795\n",
        "  forest: 0.0897\n",
        "  agriculture: 0.2445"
      ],
      "metadata": {
        "id": "lQeli8eGQfZS"
      }
    }
  ]
}